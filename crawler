#!/bin/bash

#  while [ 1 ]; do pgrep crawler_ | wc -l; sleep 1;done

MAX_THREADS=10
MAX_URLS=10

function urlCount
{
	echo -n `psql -t -A test <<EOF
SELECT COUNT(*) FROM urls WHERE status IS NULL AND crawler IS NULL;
EOF`
}

function urlGetIDs
{
	echo -n `psql -A -t test <<EOF
SELECT id FROM urls WHERE status IS NULL AND crawler IS NULL LIMIT ${MAX_URLS}
EOF` | sed s/\n//g
}

function urlCrawlIDs
{
	IDS=`echo -n "$2" | sed s/\ /,\ /g`
	echo -n `psql -A -t test <<EOF
UPDATE urls SET status = 'crawling' WHERE id IN (${IDS});
EOF` > /dev/null
}

function urlDownloadIDs
{
	IDS=`echo -n "$*" | sed s/\ /,\ /g`

	echo -n `psql -A -t test <<EOF
UPDATE urls SET status = 'crawling' WHERE id IN (${IDS});
EOF` > /dev/null
}

function urlGetURL
{
	echo -n `psql -A -t test <<EOF
SELECT url FROM urls WHERE id = $1
EOF`
}

THREAD_NAME=`echo -en "${0}" | sed s/[\.\/]//g`

if [ "${THREAD_NAME}" == "crawler" ]
then
	echo "Inicializando processo principal $0 ${PID}.."
	ITERACOES=1
	CONTADOR=0
	LIMIT=`expr ${MAX_THREADS} + 1`
	URLS="`urlCount`"
	while [ ${URLS} -gt 0 ]
	do
		threads=`expr ${MAX_THREADS} - $(pgrep crawler_ | wc -l)`
		if [ ${threads} -lt ${LIMIT} ]
		then
			for thread in `seq ${threads}`
			do
				CONTADOR=`expr ${CONTADOR} + 1`
				NEW_THREAD="${THREAD_NAME}_${CONTADOR}"
				ln -s "${0}" "${NEW_THREAD}"
				IDS="`urlGetIDs`"
				urlCrawlIDs ${IDS}
				nohup "./${NEW_THREAD}" "${IDS}" >> crawler.log&
			done
		fi
		ITERACOES=`expr ${ITERACOES} + 1`
		sleep 10
	done
else
	echo "Inicializando processo paralelo $0 baixando '$*'"
	wget -o downloads/$1 `urlGetURL $1`
	unlink $0
fi
